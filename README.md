# 🧠 CLIP Image Classification with Zero-Shot Learning

This repository contains a Jupyter Notebook demonstrating how to use OpenAI's CLIP (Contrastive Language–Image Pre-training) model for zero-shot image classification. Instead of requiring labeled datasets, CLIP can classify images using natural language text prompts.

## 🚀 Features

- 🔍 Zero-shot image classification with no model training required
- 🧠 Uses text prompts to dynamically label images
- 🖼️ Displays input image and top predicted class with confidence scores

## 📁 Files

- `CLIP_Image_Classification_with_zero_shot.ipynb` — Main notebook with step-by-step implementation

## 📦 Requirements

Install the necessary libraries before running the notebook:

```bash
pip install torch torchvision transformers matplotlib pillow
```

## Example

["a photo of a cat", "a photo of a dog", "a photo of a zebra crossing", "a photo of a bus"]

📜 Reference
CLIP Paper (OpenAI)

Hugging Face Transformers - CLIP



