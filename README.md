# ğŸ§  CLIP Image Classification with Zero-Shot Learning

This repository contains a Jupyter Notebook demonstrating how to use OpenAI's CLIP (Contrastive Languageâ€“Image Pre-training) model for zero-shot image classification. Instead of requiring labeled datasets, CLIP can classify images using natural language text prompts.

## ğŸš€ Features

- ğŸ” Zero-shot image classification with no model training required
- ğŸ§  Uses text prompts to dynamically label images
- ğŸ–¼ï¸ Displays input image and top predicted class with confidence scores

## ğŸ“ Files

- `CLIP_Image_Classification_with_zero_shot.ipynb` â€” Main notebook with step-by-step implementation

## ğŸ“¦ Requirements

Install the necessary libraries before running the notebook:

```bash
pip install torch torchvision transformers matplotlib pillow
```

## Example

["a photo of a cat", "a photo of a dog", "a photo of a zebra crossing", "a photo of a bus"]

ğŸ“œ Reference
CLIP Paper (OpenAI)

Hugging Face Transformers - CLIP



